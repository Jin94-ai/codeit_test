# Health Eat 발표자료 구조

> **발표 시간**: 15분 | **대상**: 코드잇 수강생

---

## 슬라이드 구성 (총 26장)

---

### 1. 표지 (1장)

```
Health Eat - AI 알약 인식 프로젝트

코드잇 8팀
2025.12.04 ~ 12.23

팀원: 이진석(Leader), 김민우, 김나연, 김보윤, 황유민
```

---

### 2. 목차 (1장)

```
1. 프로젝트 개요
2. 데이터 분석 (EDA)
3. 시행착오의 여정
4. 핵심 전환점: 2-Stage Pipeline
5. 최종 모델 아키텍처
6. 실험 결과
7. ⭐ 베스트 모델 상세 분석 (0.96703)
8. 팀 협업 & 비즈니스 방향성
9. 결론 및 회고
```

---

### 3. 프로젝트 개요 (1장)

**핵심 내용**:
| 항목 | 내용 |
|------|------|
| 목표 | Kaggle mAP@[0.75:0.95] 최대화 |
| 기간 | 3주 (12/04 ~ 12/23) |
| 최종 점수 | **0.96703** |
| Baseline 대비 | **+18%** 개선 |

**시각 자료**: 점수 개선 그래프
```
0.7 → 0.82 → 0.822 → 0.920 → 0.963 → 0.96703
```

---

### 4. 데이터 분석 - EDA 개요 (1장)

**데이터 구조 분석** (담당: 김민우, 김나연):

| 구분 | 원본 | 필터링 후 |
|------|------|----------|
| Train 이미지 | 651개 | **232개** |
| Test 이미지 | 843개 | - |
| 어노테이션 | 1,001개 | 763개 |
| 클래스 수 | **56개** | - |


**Classifier용 클래스 매핑** (extract_and_crop.py):

| 구분 | 클래스 수 | 설명 |
|------|----------|------|
| KAGGLE_56_DL_IDX | 56개 | Train 데이터 기준 클래스 |
| MISSING_18_DL_IDX | 18개 | Test에만 존재하는 추가 클래스 |
| **ALL_74_DL_IDX** | **74개** | 56 + 18 = 전체 타겟 클래스 |

**K-code ↔ dl_idx 변환**:
```python
# dl_idx: 내부 인덱스 (0-based)
# K-code: 약물 코드 (K-000001 ~ K-999999)

def k_code_to_dl_idx(k_code: str) -> int:
    """K-001900 → 1899 (숫자 - 1)"""
    return int(k_code.split("-")[1]) - 1

def dl_idx_to_k_code(dl_idx: int) -> str:
    """1899 → K-001900 (숫자 + 1, 6자리 패딩)"""
    return f"K-{dl_idx + 1:06d}"
```

**클래스 매칭 방식**:
```python
# ✅ 폴더 이름(K-code)으로 클래스 결정
k_code_folder = annotation_path.parent.name  # 예: "K-001900"
dl_idx = k_code_to_dl_idx(k_code_folder)     # → 1899
```

**핵심 포인트**: Classifier는 **JSON category_id가 아닌 폴더 구조**(K-code)로 클래스 매핑

---

**데이터 정합성 문제 발견**:
- 폴더에는 있지만 JSON에 없는 이미지: **419개**
- JSON에는 있지만 폴더에 없는 이미지: **137개**
- **해결**: 폴더 ∩ JSON 교집합만 사용
- 필터링 후 유효 학습 데이터: **232개 이미지, 763개 bbox**

**시각 자료**: 데이터 구성 파이차트, JSON 매핑 플로우차트

---

### 5. 데이터 분석 - EDA 상세 (1장)

**이미지 특성**:
| 항목 | 분석 결과 |
|------|----------|
| 해상도 | 976x1280 (단일 해상도) |
| 촬영 배경 | 연회색 배경 (100%) |
| 조명 | 주백색 (100%) |
| bbox 평균 area | 68,646 px² |
| 이미지당 평균 알약 | 2.7개 (최대 4개) |

**알약 메타데이터 분포**:
| 항목 | 분포 |
|------|------|
| 모양 | 원형(48%), 타원형(32%), 장방형(18%) |
| 색상 | 주황(28%), 하양(26%), 분홍(15%), 노랑(13%) |
| 방향 | 앞면(99%), 뒷면(1%) |

**클래스 불균형 (심각)**:
- 최다: **240개** (일양하이트린정 2mg, 24%)
- 최소: **3개** (0.3%)
- 비율: **1:80** (상위 vs 하위)

**시각 자료**: 클래스 분포 막대 그래프, 알약 모양/색상 파이차트

---

### 6. 인프라: Ubuntu 자동화 파이프라인 (1장)

**담당**: 김보윤 (Model Architect)

**Baseline부터 적용** - 처음부터 자동화된 파이프라인으로 실험 진행

**자동화 구조** (exc.sh → run.sh):
```
┌─────────────────────────────────────────────────────────────┐
│                    exc_pip 명령어 실행                       │
├─────────────────────────────────────────────────────────────┤
│  1. 패키지 설치                                              │
│     └── gdown, albumentations, ultralytics, kaggle          │
│  2. 데이터 로딩                                              │
│     └── python -m src.data.data_load.data_loader            │
│  3. YOLO 포맷 변환                                           │
│     └── python -m src.data.yolo_dataset.yolo_export         │
│  4. 모델 학습                                                │
│     └── python -m src.models.[모델명]                       │
└─────────────────────────────────────────────────────────────┘
```

**핵심 기능**:
| 기능 | 구현 내용 |
|------|----------|
| **Seed 고정** | Python, NumPy, PyTorch, CUDA 전체 시드 42 고정 |
| **체크포인트** | 5 에포크마다 자동 저장 (save_period=5) |
| **Early Stopping** | patience=10~15 에포크 |
| **W&B 연동** | mAP@0.75:0.95 실시간 모니터링 (callbacks.py) |

**시각 자료**: 자동화 파이프라인 아키텍처 다이어그램

---

### 7. 데이터 확장 시도 - AIHub (1장)

**외부 데이터 활용 결정**: AIHub 의약품 데이터셋

| 단계 | 데이터셋 | 작업 내용 | 결과 |
|------|---------|----------|------|
| 1차 | 복합경구제 | 56개 클래스 필터링 | 실패 (map점수 0.014 이하) |
| 2차 | 단일경구제 | 1.5TB 다운로드 (23시간) | 0.822 (미미) |
| 3차 | TL/TS 1,3,4 | 18개 추가 클래스 확보 | 74개 확장 |
| 4차 | TL/TS 1~8 | 전체 74개 클래스 데이터 | 최종 데이터셋 |

**12/17 강사님 디렉션**:
- 테스트 이미지에 **18개 추가 클래스** 존재 확인
- 복합경구제 TL/TS에서 추가 클래스 확보 제안

**결과**: 56개 + 18개 = **74개 클래스**로 확장

---

### 8. 시행착오의 여정 - 상세 타임라인 (2장)

**Phase 1: Baseline 구축**
```
보윤님 Ubuntu 자동화 파이프라인 → YOLO12m End-to-end → 0.82점
```

**Phase 2: AIHub 복합경구제 시도 (실패)**
| 문제 | 상세 내용 |
|------|----------|
| **이미지당 라벨 1개** | 학습 시 이미지당 bbox 1개만 학습 → 예측도 bbox 1개만 검출 |
| **클래스 폭증** | 이미지에 해당하는 모든 라벨 가져오니 클래스 수 급증 |
| **결과** | mAP **0.014 이하**로 급락 |

**Phase 3: AIHub 단일경구제 시도 (미미)**
| 문제 | 상세 내용 |
|------|----------|
| **데이터 특성 불일치** | 이미지당 알약 1개, 어두운 배경, 붉은색 조명 |
| **데이터셋 거대** | TB 단위 → 라벨만 다운로드 후 56개 클래스 필터링 |
| **소요 시간** | 1.5TB 다운로드 **23시간** |
| **팀 공유** | 클래스당 100개씩 추출 (6GB) |
| **결과** | Kaggle + 단일경구제 합쳐도 **0.822** (유의미하지 않음) |

**Phase 4: 74개 클래스 확장 후 ROI Crop 시도 (실패)**
| 문제 | 상세 내용 |
|------|----------|
| **Phase 2 문제 답습** | 복합경구제와 동일한 bbox 문제 재발 |
| **ROI Crop 시도** | Kaggle + AIHub 데이터 크롭, 육안 검열 |
| **치명적 오류** | bbox를 **이미지 전체**로 설정하고 학습 |
| **결과** | 알약 bbox **검출 불가** |

**Phase 5: 2-Stage Pipeline 도입 (성공!)**
```
문제 인식: End-to-end로는 한계
해결책: Detector(위치) + Classifier(분류) 분리
결과: 0.822 → 0.963 (+0.141) 대폭 상승!
```

**Phase 6: 데이터 정제 → Best Score**
```
cleanup_detector_data.py 적용
결과: 0.963 → 0.967 (Best!)
```

**핵심 교훈**:
1. **데이터 구조 이해 필수** - bbox 개수, 이미지 특성 파악
2. **도메인 불일치 주의** - 단일경구제 vs Kaggle 배경/조명 차이
3. **크롭 로직 검증** - bbox 좌표가 올바른지 확인
4. **실패를 통한 학습** - 0.014 실패가 2-Stage 아이디어의 계기

---

### 9. 핵심 전환점: 2-Stage Pipeline (1장)

**기존 방식의 한계**:
```
이미지 → YOLO → 74개 클래스 직접 예측 (End-to-end)
→ 클래스 불균형, 데이터 부족으로 성능 한계
```

**새로운 접근**:
```
이미지 → Detector → 알약 위치 검출 (단일 클래스 "Pill")
       → Classifier → 74개 클래스 분류
```

**분리의 이점**:
1. **Detector**: 위치/박싱만 담당 → 일반화 용이
2. **Classifier**: 분류만 담당 → 독립 최적화
3. 각각 최적의 모델 선택 가능

---

### 10. 최종 모델 아키텍처 (1장)

```
┌─────────────────────────────────────────────────────────────┐
│                    2-Stage Pipeline                         │
├─────────────────────────────────────────────────────────────┤
│  Stage 1: YOLO11m Detector                                  │
│  ├── Input: 원본 이미지 (1280x960)                          │
│  ├── Output: Bounding Boxes                                 │
│  ├── 클래스: 단일 ("Pill")                                  │
│  └── 데이터: Kaggle(232) + AIHub(~5,000)                    │
├─────────────────────────────────────────────────────────────┤
│  Stage 2: ConvNeXt Classifier                               │
│  ├── Input: 크롭된 알약 이미지 (224x224)                    │
│  ├── Output: K-code + Confidence                            │
│  ├── 클래스: 74개                                           │
│  └── Pretrained: ImageNet                                   │
└─────────────────────────────────────────────────────────────┘
```

---

### 11. 핵심 기술 상세 - Stage 1 (1장)

**YOLO11m Detector**

| 설정 | 값 |
|------|-----|
| 모델 | yolo11m.pt |
| imgsz | 640 |
| epochs | 50 |
| batch | 8 |

**학습 결과**:
| Metric | 값 |
|--------|-----|
| mAP50 | 0.995 |
| mAP50-95 | 0.85 |
| Precision | 0.99 |
| Recall | 0.99 |

**핵심 개선**: 데이터셋 정제
- 테스트 환경과 유사한 3~4개 bbox 이미지만 사용
- 품질 필터링으로 학습 데이터 품질 향상

---

### 12. 핵심 기술 상세 - Stage 2 (1장)

**ConvNeXt Classifier**

| 설정 | 값 |
|------|-----|
| 모델 | convnext_tiny (ImageNet pretrained) |
| img_size | 224 |
| epochs | 50 |
| lr | 1e-4 |

**선정 이유**:
1. ImageNet pretrained → 안정적인 feature extraction
2. YOLO-cls와 분류 정확도 비슷 → 변경할 이유 없어 유지
3. 크롭 이미지 분류에 최적화

**학습 결과**:
- Val Accuracy: 98.5%
- Early Stop: Epoch 21

---

### 13. W&B 실험 추적 시스템 (1장)

**담당**: 황유민 (Experiment Lead)

**역할**:
> MLflow vs W&B 선택 → **W&B 채택**
> 팀 프로젝트로 기획 → 실험 → 로그 공유 → 피드백 → 버전 컨트롤 lead

**구축 내용**:
| 항목 | 구현 내용 |
|------|----------|
| **팀 워크스페이스** | entity='codeit_team8' 설정, 팀원 전체 초대 |
| **로그 구조 정의** | logs/experiments/ 실험 결과 관리 |
| **메트릭 추적** | mAP50, mAP50-95, mAP75-95, loss 실시간 모니터링 |
| **이미지 분석** | 예측 이미지 자동 업로드 및 시각화 |
| **실험 ID 표준화** | exp_001, exp_002... 체계적 명명 규칙 |

**callbacks.py 핵심 코드** (보윤님 구현):
```python
# Kaggle 평가 기준: mAP@0.75:0.95 계산
ap_75_95 = ap[:, 5:].mean()  # IoU 0.75~0.95 (index 5~9)
log_dict["val/mAP75-95"] = float(ap_75_95)
wandb.log(log_dict)
```

**시각 자료**: W&B 대시보드 스크린샷 (학습 곡선, 메트릭 비교)

---

### 14. 실험 전략 및 실제 실험 진행 상황 (1장)

**담당**: 황유민 (Experiment Lead)

**실험 순서 설계**:
```
Data Augmentation → Hyperparameter → Model → Ensemble
```

**실험 프로토콜**:
| 항목 | 내용 |
|------|------|
| 통제 변수 | Seed=42, Epoch=50, Batch=8 |
| 비교 방법 | A/B 실험 (단일 변수 변경) |
| 기록 방식 | W&B + 실험일지 (exp_xxx_log.md) |

**1. 데이터 최적화 계획**
- 데이터 증강을 미세하게 조정할 수 있도록 담당 분리.
| 축 | 실험 내용 | 담당 |
|-----|----------|------|
| **Input Size** | 640 vs 1280 해상도 비교 | 이진석 |
| **기본 Aug** | 기본 증강 조합 실험 | 김민우 |
| **색상 증강** | 색상 변환, 밝기/대비 조정 | 김나연 |
| **기하 증강** | 회전, 이동, 스케일 변환 | 김보윤 |

**실제 적용**
- 기본적인 약·중·강 세 조합의 증강으로 실험을 해보기로 변경.
- 최종 모델 확정 이후, 성능에 리스크가 되지 않을 것으로 판단되는 기본적인 증강만 적용하기로 변경.

**2. 하이퍼 파라미터 최적화 계획**
- yolo.tune기능 이용.
- lr, scheduler -> optimizer, batch size 순으로 최적화

**실제 적용**
- yolo.tune를 활용한 튜닝을 시도하였으나, 최소 250 epochs 이상의 반복 학습이 요구, GPU 사용의 한계
- 최대 학습: 135 epochs, 그 이상은 불가능

**3. 모델 최적화 계획**
- YOLO n/s/m 규모 및 v8·v11·v12 버전별 모델을 실험

**실제 적용**
- 스케줄상의 이유로 데이터셋 정제 이전에 모델 실험 선행, 모델별 특성과 성능을 사전에 파악.


**핵심 인사이트**:
- 실험 방향은 팀원 전체의 리소스를 활용해 제한된 시간과 자원 내에서 최적의 변수 조합을 찾는 것이었으나, 프로젝트 진행 과정에서 현실적인 계획이 아님이 드러났다. 프로젝트 후반에는 경우의 수 기반의 파인 튜닝보다 모델 구조와 데이터셋 보완 등 성능 향상에 직접적인 영향을 주는 요소에 집중하게 되었다.
- 미세 조정이 필요한 경우, 팀원별로 실험을 분산하기보다 2명은 모델 실험을, 3명은 결과를 바탕으로 데이터셋 수정을 담당하는 역할 분담이 더 효율적이었다.
- 변수가 많아 사전에 수립한 스케줄대로 실험을 진행하는 데에는 한계가 있었다.
- W&B 분석은 실험 중 혼선을 줄이는 데에는 효과적이었으나, 이를 즉각적인 성능 향상으로 연결하는 데에는 어려움이 있었다.
- 모델 변경 시 최적 하이퍼파라미터는 약 10~20% 변동하였으며, 핵심 하이퍼파라미터(앵커 조합)를 먼저 고정한 뒤 fine-tuning을 수행하는 전략이 더 효율이다..



**시각 자료**: 증강 실험 전략 다이어그램, 4축 실험 테이블

---

### 15. 데이터 정제 - Best Score 달성 (1장)

**cleanup_detector_data.py**

| 정제 기준 | 값 | 이유 |
|----------|-----|------|
| bbox 개수 | 2~4개 | Kaggle 테스트와 동일 |
| IoU 임계값 | 0.7 | 진짜 중복만 검출 |
| bbox 크기 | 30px 이상 | 너무 작은 bbox 제외 |
| 종횡비 | 3.5 이하 | 비정상 형태 제외 |

**핵심 버그 수정**:
- 문제: category_id 변환 오류 (YOLO index vs dl_idx 혼동)
- 해결: `self.classifier.names[top1_idx]` 직접 사용
- 효과: 0.81 → **0.96** 달성!

---

### 16. 실험 결과 (1장)

**Kaggle 제출 히스토리**:

| # | Score | 설명 | 변화 |
|---|-------|------|------|
| 1 | 0.82 | Baseline (YOLO12m End-to-end) | - |
| 2 | 0.822 | 단일경구제 + 56클래스 | +0.002 |
| 3 | **0.920** | **2-Stage (YOLO + YOLO-cls)** | **+0.098** |
| 4 | **0.963** | **데이터셋/Detector 개선** | **+0.043** |
| 5 | 0.965 | AIHub 데이터 추가 | +0.002 |
| **6** | **0.96703** | **데이터셋 정제 + ConvNeXt** | **+0.002** |

**시각 자료**: 점수 추이 그래프

---

### 17. 베스트 모델 상세 분석 (1장)

**0.96703 달성 - 핵심: 데이터셋 정제**

**정제 기준** (cleanup_detector_data.py):
```python
BBOX_COUNT_RANGE = (3, 4)  # Kaggle 테스트와 동일
IOU_THRESHOLD = 0.7        # 중복 제거
MIN_BBOX_SIZE = 50         # 너무 작은 bbox 제외
MAX_BBOX_SIZE = 500        # 너무 큰 bbox 제외
```

**핵심 인사이트**:
- 테스트 환경과 유사한 **3~4개 bbox 이미지만** 선별
- 품질 낮은 데이터 제거 → 학습 효율 향상
- Classifier 모델(ConvNeXt vs YOLO-cls)은 성능 차이 없음

**정제 결과**:
| 항목 | 정제 전 | 정제 후 |
|------|--------|--------|
| 이미지 수 | ~10,000개 | ~7,000개 |
| 품질 | 혼재 | 고품질만 |

**재현 명령어**:
```bash
python src/data/aihub/extract_for_detector.py # Detector 데이터 추출
python src/data/cleanup_detector_data.py --delete  # 데이터 정제
python src/models/yolo11m_detector.py  # Detector 학습
python src/data/aihub/extract_and_crop.py # Classifier 데이터 추출 & 정제
python src/models/convnext_classifier.py  # Classifier 학습
python -m src.inference.submit_v2 --det_conf 0.05 --cls_conf 0.3
```

**시각 자료**: 정제 전후 데이터 분포 비교

---

### 18. 실패 사례와 교훈 (1장)

| 시도 | 결과 | 교훈 |
|------|------|------|
| AIHub 복합경구제 | bbox 1개만 검출 | 데이터 구조 이해 필수 |
| ROI Crop | 검출 불가 | 크롭 로직 검증 필요 |
| imgsz 1280 | 0.713 | 학습/추론 설정 일치 중요 |
| TTA 적용 | 0.533 | mAP@[0.75:0.95]에서 bbox 정밀도 저하 |

**핵심 교훈**:
1. 데이터 품질 > 데이터 양
2. 실패도 중요한 학습 기회
3. 빠른 실험 → 빠른 피드백 → 방향 전환

---

### 19. 팀 협업 & 역할 분담 (1장)

**12/18 팀 회의 - 비즈니스 방향성 논의**

| 담당자 | 역할 | 주요 기여 |
|--------|------|----------|
| **이진석** | Leader / 모델 구현 | 2-Stage Pipeline 설계/구현 |
| **김민우** | 서비스 타당성 검토 | B2C/B2B 비즈니스 모델 분석 |
| **김나연** | B2B 시장 조사 | 약국/응급구조대/수사기관 니즈 조사 |
| **김보윤** | 자동화 / 배포 | Ubuntu 파이프라인 자동화, W&B 콜백, Cloud 아키텍처 |
| **황유민** | W&B / 실험 설계 | W&B 실험 추적 시스템 구축, 증강 실험 전략 설계 |

**협업 통계**:
| 항목 | 수치 |
|------|------|
| 총 커밋 | ~180개 |
| PR | 86개 |
| 협업일지 | 40개 |
| 실험 로그 | 12개 |

---

### 20. 비즈니스 방향성 (1장)

**프로젝트 배경** (12/18 팀회의):
> "유저가 모바일 앱으로 복용 중인 약 사진을 찍으면, 해당 약에 대한 정보를 제공"

**타겟 시장 분석**:

| 시장 | 타겟 | 가치 제안 |
|------|------|----------|
| **B2C** | 고령자 (다약제 복용) | 알약 식별, 효능 안내 |
| **B2B** | 응급구조대, 수사기관 | 비전문가의 신속한 약물 식별 |

**결론**: B2C 공익성 있으나, **B2B SaaS 모델 병행** 권장

---

### 21. 서비스 아키텍처 (1장)

**Cloud 방식 채택** (On-Device 대비 장점):

| 항목 | On-Device | Cloud (채택) |
|------|-----------|--------------|
| 앱 크기 | ~100MB | **~20MB** |
| 모델 업데이트 | 앱 재배포 필요 | 서버만 수정 |
| 기기 호환성 | 고사양 필요 | 저사양 가능 |

```
📱 Mobile App (Thin Client)  ───▶  ☁️ Cloud Server (YOLO + GPU)
     ~20MB, CameraX                    FastAPI + PostgreSQL
```

**Uncertainty Handling** (의료 AI 철칙):
| Confidence | 상태 | 액션 |
|------------|------|------|
| >= 0.9 | CONFIRMED | 결과 표시 |
| 0.7 ~ 0.9 | UNCERTAIN | "다시 촬영해주세요" |
| < 0.7 | UNKNOWN | "약사에게 문의하세요" |

---

### 22. Android 앱 개발 시도 (1장)

**담당**: 김보윤 (Model Architect)

**목표**: YOLO 모델을 모바일에서 직접 실행 (On-Device 검증)

**1. 모델 변환**
- `.pt` → `.pte` (ExecuTorch 사용)
```python
from ultralytics import YOLO
detector = YOLO("best_detector.pt")
detector.export(format="executorch")
```

**2. Android 앱 구조**
```
app/
├── java/co.kr.medcam_re/
│   ├── ml/
│   │   ├── YoloDetector      # YOLO 검출기
│   │   ├── Classifier        # 분류 모델
│   │   └── YoloPostProcessor # 후처리
│   └── MainActivity          # 카메라 연동
└── assets/
    ├── best_detector.pte     # 검출 모델
    └── best_classifier.pte   # 분류 모델
```

**3. 실행 결과**
| 항목 | 결과 |
|------|------|
| 알약 검출 | 성공 (K-001900, 신뢰도 78%) |
| bbox 표시 | 정상 동작 |
| 다중 알약 | 동시 검출 가능 |

**4. 한계점 & 결론**
- ExecuTorch 변환 시 일부 연산자 미지원
- 2-Stage Pipeline 모바일 구현 복잡
- **결론**: Cloud 방식 채택 (슬라이드 21)

**시각 자료**: 앱 실행 스크린샷 (단일/다중 알약 인식)

---

### 24. 프로젝트 타임라인 (1장)

```
Phase 1-2 (12/04~09): 셋업, EDA, 56개 클래스 확인
    ↓
Phase 3 (12/09~12): Baseline 0.82 제출 (YOLO12m)
    ↓
Phase 4 (12/11~15): AIHub 시도 - 복합경구제 실패(0.6), 단일경구제(0.822)
    ↓
Phase 5 (12/17~18): 강사님 디렉션, 18개 추가 클래스 → 74개
    ↓
Phase 6 (12/18): 2-Stage Pipeline → 0.920 → 0.963
    ↓
Phase 7 (12/18~19): 데이터 정제 → 0.96703 (Best!)
    ↓
Phase 8 (12/19~22): 비즈니스 방향성 논의, 추가 실험, 마무리
```

**핵심 전환점**: 12/18 - 2-Stage Pipeline 도입 & 팀 회의

---

### 25. 핵심 성공 요인 (1장)

1. **2-Stage 분리** (+0.10)
   - Detection과 Classification 독립 최적화
   - 각 Task에 맞는 모델 선택

2. **ConvNeXt 유지**
   - ImageNet pretrained → 안정적인 feature extraction
   - YOLO-cls와 분류 정확도 비슷하여 변경 불필요

3. **데이터 정제** (+0.004)
   - 테스트 환경과 유사한 2~4개 bbox 이미지만 사용
   - 모든 알약 bbox 수집

4. **자동화 파이프라인** (김보윤)
   - exc.sh → run.sh 단일 명령어 실행
   - Seed 고정, 체크포인트, W&B 실시간 모니터링

5. **W&B 실험 추적 시스템** (황유민)
   - 팀 워크스페이스 구축, 메트릭 실시간 모니터링
   - 체계적인 실험 관리 (exp_001, exp_002...)

6. **증강 실험 전략** (황유민)
   - Data Aug → Hyperparameter → Model → Ensemble 순서
   - 4가지 축 (Input Size, 기본 Aug, 색상, 기하) 분업

---

### 26. 결론 & Q&A (1장)

**최종 성과**:
```
Baseline 0.82 → Best 0.96703 (+18%)
```

**핵심 메시지**:
1. **End-to-end의 한계 인식** → 2-Stage 전환
2. **수많은 시행착오**가 성공의 밑거름
3. **데이터 품질 > 데이터 양**
4. **비즈니스 방향성 고민** → B2B SaaS 모델 검토

```
감사합니다. 질문 있으시면 말씀해 주세요.

GitHub: github.com/Jin94-ai/codeit_team8_project1
```

---

## 발표 시간 배분 (15분)

| 섹션 | 슬라이드 | 시간 |
|------|---------|------|
| 프로젝트 개요 & EDA | 1-5 | 2분 |
| 자동화 파이프라인 | 6 | 1분 |
| 데이터 확장 & 시행착오 | 7-8 (2장) | 2분 |
| 2-Stage 전환 | 9-10 | 1.5분 |
| 기술 상세 | 11-12 | 1.5분 |
| W&B & 증강 실험 | 13-14 | 1분 |
| 데이터 정제 & 결과 | 15-16 | 1.5분 |
| ⭐ **베스트 모델 상세** | **17** | **1.5분** |
| 실패 사례 | 18 | 0.5분 |
| 팀 협업 & 비즈니스 | 19-21 | 1.5분 |
| Android 앱 개발 시도 | 22 | 0.5분 |
| 결론 | 24-26 | 1분 |

---

## 주요 시각 자료 (권장)

1. **EDA 시각화** (notebooks/eda.ipynb, mw_eda.ipynb, ny_eda.ipynb 참고):
   - 데이터 구성 파이차트 (Train 651개, Test 843개, Annotation 1001개)
   - 클래스 분포 막대 그래프 (Top-10: 일양하이트린정 240개 → 최소 3개)
   - 알약 모양 분포 (원형 48%, 타원형 32%, 장방형 18%)
   - 알약 색상 분포 (주황 28%, 하양 26%, 분홍 15%)
   - 이미지당 bbox 개수 분포 (평균 2.7개)
2. **점수 추이 그래프**: 0.82 → 0.6 → 0.822 → 0.920 → 0.963 → 0.96703
3. **2-Stage 아키텍처 다이어그램**: Detector → Classifier 흐름
4. **실패 사례 타임라인**: 복합경구제 실패 → 단일경구제 → ROI Crop 실패 → 2-Stage 성공
5. **협업 통계**: 커밋/PR/일지 숫자
6. **데이터 변천**: 56개 → 74개 클래스
7. **자동화 파이프라인 다이어그램**: exc.sh → run.sh → 학습 흐름
8. **W&B 대시보드 스크린샷**: 학습 곡선, 메트릭 비교
9. **증강 실험 전략 다이어그램**: 4가지 축 실험 테이블
10. **Android 앱 스크린샷**: 단일/다중 알약 인식 결과 (images/app_dev/)

---

## 예상 질문 & 답변

### Q1. 왜 2-Stage를 선택했나요?
> End-to-end 방식에서 클래스 불균형과 데이터 부족으로 한계에 부딪혔습니다.
> Detector를 단일 클래스로 학습하면 모든 알약 위치를 검출할 수 있고,
> Classifier는 74개 클래스 분류에만 집중할 수 있어 각각 최적화가 가능합니다.

### Q2. ConvNeXt를 선택한 이유는?
> ImageNet pretrained 모델로 강력한 특징 추출이 가능합니다.
> YOLO-cls와 분류 정확도는 비슷했지만, ImageNet pretrained 모델로 안정적인 feature extraction이 가능하여 유지했습니다.

### Q3. AIHub 데이터는 어떻게 활용했나요?
> Detector와 Classifier 모두에 사용했습니다.
> Detector는 알약 위치 검출용 bbox 데이터로, Classifier는 74개 클래스 분류용 크롭 이미지로 활용했습니다.

### Q4. 가장 큰 성능 향상 요인은?
> 2-Stage Pipeline 도입입니다. (+0.10)
> 이후 데이터셋을 잘 정제한 것이 추가 점수 향상에 기여했습니다. Classifier 모델 자체(ConvNeXt vs YOLO-cls)는 성능 차이가 없었습니다.

### Q5. 실패 사례에서 배운 점은?
> AIHub 복합경구제에서 이미지당 bbox 1개만 추출하여 성능이 0.6까지 떨어졌습니다.
> 데이터 구조를 정확히 이해하지 못한 결과였고, 이후 모든 데이터 처리에서
> 구조 파악을 먼저 하는 습관을 들이게 되었습니다.

---

## 발표 스크립트 핵심 포인트

### 오프닝 (30초)
"안녕하세요, 코드잇 8팀 Health Eat 프로젝트 발표를 시작하겠습니다.
저희 팀은 3주간 AI 알약 인식 프로젝트를 진행하여
Baseline 0.82에서 0.96703까지 0.147만큼 성능 개선을 달성했습니다."

### 핵심 메시지 (반복)
"저희 프로젝트의 핵심은 **수많은 시행착오를 통해 End-to-end의 한계를 인식**하고
**2-Stage Pipeline으로 전환**한 것입니다."

### 클로징 (30초)
"감사합니다. 질문 있으시면 말씀해 주세요."

---

## 파일 생성 완료

이 문서를 기반으로 PPT 또는 PDF 발표자료를 제작하시면 됩니다.
