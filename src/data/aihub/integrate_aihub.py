#!/usr/bin/env python3
"""
AIHub ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
1. AIHubì—ì„œ TL ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (aihubshell ì‚¬ìš©)
2. ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python integrate_aihub.py
3. ìë™ìœ¼ë¡œ ì••ì¶• í•´ì œ â†’ í•„í„°ë§ â†’ JSON ìˆ˜ì •
4. ê²°ê³¼: data/aihub_integrated/annotations/
"""
import json
import zipfile
from pathlib import Path
from collections import defaultdict

TARGET_CLASSES = {
    '1899', '2482', '3350', '3482', '3543', '3742', '3831', '4542',
    '12080', '12246', '12777', '13394', '13899', '16231', '16261', '16547',
    '16550', '16687', '18146', '18356', '19231', '19551', '19606', '19860',
    '20013', '20237', '20876', '21324', '21770', '22073', '22346', '22361',
    '24849', '25366', '25437', '25468', '27732', '27776', '27925', '27992',
    '28762', '29344', '29450', '29666', '30307', '31862', '31884', '32309',
    '33008', '33207', '33879', '34596', '35205', '36636', '38161', '41767'
}

MAX_PER_CLASS = 200  # í´ë˜ìŠ¤ë‹¹ ìµœëŒ€ ê°œìˆ˜


def analyze_competition():
    """Competition ë°ì´í„° ë¶„ì„"""
    print("\n[1/3] Competition ë°ì´í„° ë¶„ì„")
    comp_anno = Path("data/train_annotations")
    counts = defaultdict(int)

    if comp_anno.exists():
        for json_path in comp_anno.glob("*.json"):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if 'categories' in data and data['categories']:
                    cat_id = str(data['categories'][0]['id'])
                    if cat_id in TARGET_CLASSES:
                        counts[cat_id] += 1
            except:
                continue

    lacking = sum(1 for c in TARGET_CLASSES if counts.get(c, 0) < 10)
    print(f"  Competition í´ë˜ìŠ¤: {len(counts)}/56")
    print(f"  10ê°œ ë¯¸ë§Œ í´ë˜ìŠ¤: {lacking}ê°œ")
    return counts


def extract_zips():
    """ë‹¤ìš´ë¡œë“œëœ ZIP íŒŒì¼ ì••ì¶• í•´ì œ"""
    print("\n[2/3] ZIP íŒŒì¼ ì••ì¶• í•´ì œ")

    base_dir = Path("data/166.ì•½í’ˆì‹ë³„ ì¸ê³µì§€ëŠ¥ ê°œë°œì„ ìœ„í•œ ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°/01.ë°ì´í„°/1.Training/ë¼ë²¨ë§ë°ì´í„°")
    extract_base = Path("data/aihub_downloads")
    extract_base.mkdir(parents=True, exist_ok=True)

    # ì¡°í•©ë§Œ ì²˜ë¦¬
    zip_dirs = [
        (base_dir / "ê²½êµ¬ì•½ì œì¡°í•© 5000ì¢…", "_ì¡°í•©", "_combo"),
    ]

    total_extracted = 0

    for zip_dir, suffix, folder_suffix in zip_dirs:
        if not zip_dir.exists():
            print(f"  âš ï¸ {zip_dir.name} í´ë” ì—†ìŒ")
            continue

        for zip_file in zip_dir.glob(f"TL_*{suffix}.zip"):
            name = zip_file.stem.replace(suffix, "")
            extract_dir = extract_base / f"{name}{folder_suffix}"

            if extract_dir.exists() and any(extract_dir.rglob("*.json")):
                print(f"  âœ“ {name}{suffix} ì´ë¯¸ ì••ì¶• í•´ì œë¨")
                continue

            try:
                print(f"  ì••ì¶• í•´ì œ ì¤‘: {zip_file.name}")
                with zipfile.ZipFile(zip_file, 'r') as zf:
                    zf.extractall(extract_dir)
                print(f"  âœ… {name}{suffix} ì™„ë£Œ")
                total_extracted += 1
            except Exception as e:
                print(f"  âŒ {name}{suffix} ì‹¤íŒ¨: {e}")

    if total_extracted == 0:
        print("  (ëª¨ë‘ ì´ë¯¸ ì••ì¶• í•´ì œë¨)")


def integrate_aihub(comp_counts):
    """AIHub ë°ì´í„° í•„í„°ë§ ë° í†µí•©"""
    print("\n[3/3] AIHub ë°ì´í„° í†µí•©")

    output_dir = Path("data/aihub_integrated/annotations")
    output_dir.mkdir(parents=True, exist_ok=True)

    aihub_counts = defaultdict(int)
    total = 0

    # ğŸ”§ ê³ ìœ í•œ image_id ìƒì„± (Competition IDì™€ ì¶©ëŒ ë°©ì§€)
    next_image_id = 100000

    # ğŸ”§ file_name â†’ image_id ë§¤í•‘ (ê°™ì€ ì´ë¯¸ì§€ëŠ” ê°™ì€ ID ì‚¬ìš©)
    filename_to_imageid = {}

    # TL combo + single ë°ì´í„° ì²˜ë¦¬
    aihub_dir = Path("data/aihub_downloads")
    if not aihub_dir.exists():
        print("  âš ï¸ AIHub ë°ì´í„° ì—†ìŒ")
        return aihub_counts

    # ì¡°í•©ë§Œ ì²˜ë¦¬
    for tl_dir in sorted(aihub_dir.glob("TL_*_combo")):
        print(f"  ì²˜ë¦¬ ì¤‘: {tl_dir.name}")

        # ì¡°ê¸° ì¢…ë£Œ: ëª¨ë“  í´ë˜ìŠ¤ê°€ MAX_PER_CLASS ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸
        all_filled = all(
            comp_counts.get(c, 0) + aihub_counts.get(c, 0) >= MAX_PER_CLASS
            for c in TARGET_CLASSES
        )
        if all_filled:
            print("  âœ… ëª¨ë“  í´ë˜ìŠ¤ 200ê°œ ì±„ì›Œì§, ì¡°ê¸° ì¢…ë£Œ")
            break

        processed_in_dir = 0
        for json_path in tl_dir.rglob("*.json"):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if 'images' not in data or not data['images']:
                    continue

                img_info = data['images'][0]
                dl_idx = str(img_info.get('dl_idx', ''))

                if dl_idx not in TARGET_CLASSES:
                    continue

                # í´ë˜ìŠ¤ë‹¹ MAX_PER_CLASSê°œê¹Œì§€ë§Œ
                current = comp_counts.get(dl_idx, 0) + aihub_counts.get(dl_idx, 0)
                if current >= MAX_PER_CLASS:
                    continue

                # ğŸ”§ ê°™ì€ file_nameì€ ê°™ì€ image_id ì‚¬ìš©
                img_filename = img_info.get('file_name', '')
                if img_filename not in filename_to_imageid:
                    filename_to_imageid[img_filename] = next_image_id
                    next_image_id += 1

                unique_image_id = filename_to_imageid[img_filename]

                # JSON ìˆ˜ì •
                dl_name = img_info.get('dl_name', 'Drug')
                cat_id = int(dl_idx)

                # ğŸ”§ images sectionì˜ id ì—…ë°ì´íŠ¸
                data['images'][0]['id'] = unique_image_id

                data['categories'] = [{
                    'supercategory': 'pill',
                    'id': cat_id,
                    'name': dl_name
                }]

                # ğŸ”§ annotationsì˜ image_id ì—…ë°ì´íŠ¸
                for anno in data['annotations']:
                    anno['category_id'] = cat_id
                    anno['image_id'] = unique_image_id

                # ì €ì¥
                out_name = f"{dl_idx}_{aihub_counts[dl_idx]:04d}.json"
                with open(output_dir / out_name, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

                aihub_counts[dl_idx] += 1
                total += 1
                processed_in_dir += 1

                if total % 100 == 0:
                    print(f"    ì§„í–‰: {total}ê°œ (í˜„ì¬: {len(aihub_counts)}ê°œ í´ë˜ìŠ¤)")

            except:
                continue

        print(f"    â†’ {tl_dir.name}ì—ì„œ {processed_in_dir}ê°œ ì¶”ê°€")

    print(f"  âœ… {total}ê°œ íŒŒì¼ í†µí•© ì™„ë£Œ")
    return aihub_counts


def print_summary(comp_counts, aihub_counts):
    """ê²°ê³¼ ìš”ì•½"""
    print("\n" + "=" * 70)
    print("í†µí•© ê²°ê³¼")
    print("=" * 70)

    total_img = sum(comp_counts.values()) + sum(aihub_counts.values())
    avg = total_img / 56 if total_img > 0 else 0
    lacking = sum(1 for c in TARGET_CLASSES
                  if comp_counts.get(c, 0) + aihub_counts.get(c, 0) < 10)

    print(f"Competition: {sum(comp_counts.values())}ê°œ")
    print(f"AIHub ì¶”ê°€: {sum(aihub_counts.values())}ê°œ")
    print(f"ì´ ì´ë¯¸ì§€: {total_img}ê°œ (í‰ê·  {avg:.1f}ê°œ/í´ë˜ìŠ¤)")
    print(f"10ê°œ ë¯¸ë§Œ í´ë˜ìŠ¤: {lacking}ê°œ")

    if lacking > 0:
        print(f"\nì¶”ê°€ ë‹¤ìš´ë¡œë“œ í•„ìš”: TL_11 ì´ìƒ ë°ì´í„°ì…‹")
    else:
        print(f"\nâœ… ëª¨ë“  í´ë˜ìŠ¤ 10ê°œ ì´ìƒ í™•ë³´")

    print(f"\ní†µí•© ë°ì´í„°: data/aihub_integrated/annotations/")
    print("=" * 70)


def main():
    print("=" * 70)
    print("AIHub ë°ì´í„° í†µí•©")
    print("=" * 70)

    comp_counts = analyze_competition()
    extract_zips()
    aihub_counts = integrate_aihub(comp_counts)
    print_summary(comp_counts, aihub_counts)


if __name__ == "__main__":
    main()
